{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-27T03:06:06.204714Z",
     "end_time": "2023-06-27T03:06:06.212236Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:  # np.random.uniform(0, 1) include 0 but not include 1\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "\n",
    "def update(Q, state, action, reward, state2, action2, alpha, gamma):\n",
    "    increment = reward + gamma * Q[state2, action2] - Q[state, action]\n",
    "    Q[state, action] = Q[state, action] + alpha * increment"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T03:06:06.213716Z",
     "end_time": "2023-06-27T03:06:06.227582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def sarsa(Q, epsilon, alpha, gamma, total_episodes, max_steps):\n",
    "    for episode in range(total_episodes):\n",
    "        t = 0\n",
    "        state1 = env.reset()[0]\n",
    "        action1 = choose_action(state1, epsilon)\n",
    "        while t < max_steps:\n",
    "            # Getting the next state\n",
    "            state2, reward, is_truncated, is_finished, prob = env.step(action1)\n",
    "            # If there is a hole, the transition probability of walking into a hole = 0\n",
    "            # keep the state unchanged\n",
    "            if is_truncated and reward == 0:\n",
    "                env.P[state1][action1][0] = (1.0, state1, 0.0, False)\n",
    "                break\n",
    "            # Choosing the next action\n",
    "            action2 = choose_action(state2, epsilon)\n",
    "            update(Q, state1, action1, reward, state2, action2, alpha, gamma)\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            t += 1\n",
    "            # (for test)If reaching the terminal state\n",
    "            # if reward == 1 and is_truncated:\n",
    "            #     print(episode)\n",
    "            #     print(Q)\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T03:06:06.220812Z",
     "end_time": "2023-06-27T03:06:06.249166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_optimal_policy(Q):\n",
    "    env_new = gym.make(\"FrozenLake-v1\", render_mode = \"human\", is_slippery=False)\n",
    "    state = env_new.reset()[0]\n",
    "    optimal_policy = []\n",
    "    while True:\n",
    "        optimal_action = np.argmax(Q[state, :])\n",
    "        optimal_policy.append(optimal_action)\n",
    "        state, reward, is_truncated, is_finished, prob = env_new.step(optimal_action)\n",
    "        if reward == 1 and is_truncated:\n",
    "            return optimal_policy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T03:06:06.230106Z",
     "end_time": "2023-06-27T03:06:06.250363Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q value after Sarsa method\n",
      "[[0.23292494 0.272599   0.22702243 0.22947438]\n",
      " [0.22460372 0.23273751 0.26209873 0.22384481]\n",
      " [0.22398541 0.31803239 0.22934295 0.28237667]\n",
      " [0.24235771 0.23764444 0.22635399 0.23752137]\n",
      " [0.29574451 0.35925586 0.29345126 0.22505131]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33803015 0.41221205 0.36992291 0.27916388]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.36140443 0.32523305 0.42378855 0.28213433]\n",
      " [0.34447432 0.57960295 0.45365781 0.41773757]\n",
      " [0.45126679 0.54470332 0.42467018 0.35525506]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.53207863 0.494543   0.71014097 0.42031912]\n",
      " [0.48252592 0.56787356 1.         0.46858195]\n",
      " [0.         0.         0.         0.        ]]\n",
      "The optimal policy is: \n",
      "[1, 1, 2, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "epsilon = 0.9\n",
    "alpha = 0.2  # control the increment\n",
    "gamma = 0.95  # the decay of reward\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "Q_value = sarsa(Q, epsilon, alpha, gamma, total_episodes=500, max_steps=50)\n",
    "print(\"The Q value after Sarsa method\")\n",
    "print(Q_value)\n",
    "optimal_policy = get_optimal_policy(Q_value)\n",
    "print(\"The optimal policy is: \")\n",
    "print(optimal_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T03:06:06.243589Z",
     "end_time": "2023-06-27T03:06:08.712769Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
